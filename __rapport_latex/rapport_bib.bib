@misc{Caballar2024LLM,
  author       = {Caballar, R. and Stryker, C.},
  title        = {{LLM APIs: Tips for bridging the gap}},
  year         = {2024},
  month        = {dec},
  day          = {13},
  publisher    = {IBM},
  url          = {https://www.ibm.com/think/insights/llm-apis}
}

@misc{Mazraeh2025Comprehensive,
  author       = {Mazraeh, Adnan},
  title        = {{A comprehensive guide to dimensionality reduction: From basic to super-advanced techniques 1}},
  year         = {2025},
  month        = {feb},
  day          = {23},
  publisher    = {Medium},
  url          = {https://medium.com/@adnan.mazraeh1993/a-comprehensive-guide-to-dimensionality-reduction-from-basic-to-super-advanced-techniques-1-d17ce8e734d8}
}

@article{Odden2024Using,
  author       = {Odden, T. O. B. and Tyseng, H. and Mjaaland, J. T. and Kreutzer, M. F. and Malthe-SÃ¸renssen, A.},
  title        = {{Using text embeddings for deductive qualitative research at scale in physics education}},
  journal      = {Physical Review Physics Education Research},
  year         = {2024},
  volume       = {20},
  number       = {2},
  articleno    = {020151},
  doi          = {10.1103/PhysRevPhysEducRes.20.020151}
}

@article{Radev2004Centroid,
  author       = {Radev, Dragomir R. and Jing, Hongyan and Sty, Malgorzata and Tam, Daniel},
  title        = {{Centroid-based summarization of multiple documents}},
  journal      = {Information Processing \& Management},
  year         = {2004},
  volume       = {40},
  number       = {6},
  pages        = {919--938},
  doi          = {10.1016/j.ipm.2003.10.006}
}

@article{Stewart2022Implementation,
  author       = {Stewart, G. and Al-Khassaweneh, M.},
  title        = {{An implementation of the HDBSCAN* clustering algorithm}},
  journal      = {Applied Sciences},
  year         = {2022},
  volume       = {12},
  number       = {5},
  articleno    = {2405},
  doi          = {10.3390/app12052405}
}

@misc{IBMndPrincipal,
  author       = {{IBM}},
  title        = {{Principal component analysis}},
  publisher    = {IBM},
  url          = {https://www.ibm.com/think/topics/principal-component-analysis},
  note         = {Accessed: 2025-08-09}
}

@misc{CoenennpUnderstanding,
  author       = {Coenen, A. and Pearce, A.},
  title        = {{Understanding UMAP}},
  publisher    = {People + AI Research (PAIR)},
  url          = {https://pair-code.github.io/understanding-umap/},
  note         = {Accessed: 2025-08-09}
}

@inproceedings{Beltagy2019SciBERTAP,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202558505}
}
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}|
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}
@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
