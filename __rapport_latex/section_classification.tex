% !TEX root = rapport_root.tex
\section{Section Type Classification}

To analyze theoretical and methodological trends in the PRPER corpus, we first
had to classify each of the 7313 sections by its section type. Since many
sections have ambiguous or non-standard headings, a simple keyword search is
insufficient. We therefore developed and evaluated several classification
methods.

This section details that process. First, we describe the benchmark dataset used
to evaluate performance. Second, we present the different classification models
we tested: a simple weighted heuristic method, a fine-tuned BERT model, and a
large language model (LLM) approach using the OpenAI API. Finally, we discuss
the trade-offs between these methods and justify our choice of model for the
subsequent analysis.

\subsection{Benchmarking}

To make meaningful judgments about the accuracy of each approach, we generated
two labeled benchmarks. The main dataset was generated by sampling 22 articles
with a total of 127 sections and hand-labeling them by their type. This provided
a representative, albeit small, set of sections covering a broad range of types.

To supplement this, we generated a larger secondary dataset of approximately 500
sections by selecting those with unambiguous headings (e.g., "Methods,"
"Introduction"). While this set is easier to classify, it allowed us to test our
models on a more robust number of samples. For evaluation, we developed a
function to compute overall accuracy, per-category metrics, and a confusion
matrix to diagnose errors.


\subsection{Classification with Heuristics}

As a baseline, we developed a simple heuristic classifier. We expected that a
section's type could be inferred from features like keywords in its title,
keywords in its content, its relative length, and its position within the
article. We combined these features into a weighted function to predict the
section type. After tuning, we found that the section title was by far the most
powerful predictor, with an optimal weight of 0.8. The heavy reliance on this
single feature indicates that other signals like section content or length were
not, on their own, strong predictors. Consequently, while this heuristic model
performs reasonably on sections with standard headings (e.g., "Introduction,"
"Methods"), it is inherently brittle and fails when encountering the many
sections with more creative or non-standard titles.


\input{fine_tune_and_geometric_approach}


\subsection{Classification with Large Language Models (LLMs)}

We next tested a classification approach using OpenAI's API. After experimenting
with prompts and context, we found the most effective and efficient method was
to pass the entire text of an article at once, asking the model to return a JSON
object classifying all of its sections in a single API call. This approach was
approximately seven times faster and, in addition, more accurate than
classifying sections individually.

Using this method with the `gpt-4o` model, we achieved an accuracy of \emph{90\%}
on our hand-labeled benchmark dataset. The model achieved near-perfect accuracy
on the simpler heading-based dataset. The confusion matrix (Figure
\ref{fig:conf_matrix}) reveals that most misclassifications occur between
categories that are often conceptually similar, such as mistaking a `Theoretical
Framework` for a `Literature Review`, or confusing `Implications` with
`Conclusion`.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{media/confussion_matrix_sections.png}
    \caption{Confusion matrix for the LLM-based section classification on our hand-labeled benchmark dataset.}
    \label{fig:conf_matrix}
\end{figure}


\subsection{Discussion of Classification Methods}

Our experiments evaluated four distinct classification methods, revealing a clear trade-off between performance, computational cost, and implementation complexity. The final accuracy of each model on a comparable benchmark dataset is summarized in Table~\ref{tab:accuracy_summary}.


\begin{table}%[h!]
\centering
\caption{Classification accuracy across different methods on the hand-labeled benchmark.}
\label{tab:accuracy}
\begin{tabular}{lc}
    \toprule
    \textbf{Classification method} & \textbf{Accuracy} \\
    \midrule
    Heuristics             & $\sim$0.70 \\
    \addlinespace
    Fine-tuned SciBERT & 0.77 \\
     Projection-Based MLP & 0.80 \\
    \addlinespace
    LLM (gpt-4.1)          & $\sim$0.90 \\
    LLM (gpt-4.1-mini)     & $\sim$0.80 \\
    \bottomrule
\end{tabular}
\end{table}

The simple \emph{heuristic model} served as a useful baseline but proved too brittle for this task. Among the locally run models, the fine-tuned \emph{SciBERT} represents a standard deep learning approach, achieving a respectable accuracy of \emph{0.77}, but at the cost of significant training time ($\sim$7 hours). In contrast, our novel \emph{projection-based MLP classifier} emerged as a powerful and lightweight alternative. It not only surpassed the SciBERT model with an accuracy of 0.80 but did so with a fraction of the computational cost (training in under 10 seconds) and greater data efficiency. Its primary dependency is the availability of high-quality, pre-computed text embeddings.

Finally, the \emph{LLM-based approach} using an external API delivered the highest performance, with an accuracy of approximately \emph{$\sim$0.90}. Its main drawbacks are the monetary cost of API calls and the reliance on a closed-source service, which is unsuitable for datasets with sensitive data.

Crucially, the errors made by the top-performing models often highlight genuine ambiguity in the source material. Sections titled "Discussion and Conclusion" are conceptually overlapping, and the line between a "Literature Review" and a "Theoretical Framework" can be blurry even for a human reader. This suggests that the \emph{$\sim$0.90} accuracy achieved by the LLM is approaching the practical ceiling for this classification task with this set of discrete categories.

Given its state-of-the-art performance and our goal of obtaining the highest quality classifications for our subsequent trend analysis, we selected the \emph{LLM-based classifier} for processing the entire dataset. However, our results show that the projection-based MLP is a highly compelling local alternative, offering competitive performance with minimal computational overhead.

\subsection{Exploring Inter-Section Vector Arithmetic}

Inspired by classic word embedding analogies like `king - man + woman \approx
queen`, we briefly explored whether similar vector arithmetic could model the
transitions between section types (e.g., from `Theory` to `Methods`).
Our initial, na\"{\i}ve attempt involved calculating a 'transition vector' by
subtracting a theory section's embedding from its corresponding methods
section's embedding.

This simple implementation did not yield clearly predictive results. We suspect
this is because the powerful 'thematic signal' of an entire article can easily
obscure the more subtle 'section type signal' that defines a section's function.
While a more sophisticated approach might yet prove fruitful, a deeper
investigation of this specific technique fell outside the scope of the current
project.