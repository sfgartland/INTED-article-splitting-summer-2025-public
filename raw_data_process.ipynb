{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import voyageai\n",
    "\n",
    "# We have some helper functions that lets us save results every x rows and recombine them later.\n",
    "# This lets us run large jobs wihout losing all progress in the case of a crash.\n",
    "from python_code.helpers import apply_in_chunks, recombine_chunks, embed_w_token_manager, safe_load_env_variable\n",
    "\n",
    "from python_code.analyzing import analyze_section_data_with_details, print_analysis_report, compare_text_lengths, analyze_section_content_vs_fulltext_deviation\n",
    "\n",
    "\n",
    "# Enable tqdm progress_apply for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the API_KEY using a safe helper function, this checks if \n",
    "# it uses a system env variable and sends a warning if you might \n",
    "# be using your personal api key.\n",
    "VOYAGE_API_KEY = safe_load_env_variable(\"VOYAGE_API_KEY\")\n",
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loads the data and filters it\n",
    "unfiltered_df = pd.read_pickle(\"../article_splitting_shared/prper_prstper_metadata_texts_20250227.pkl\")\n",
    "\n",
    "# Adds a year column to the dataframe\n",
    "unfiltered_df[\"year\"] = unfiltered_df[\"date\"].dt.year\n",
    "\n",
    "# Filters out the types of entries we are interested in and only includes completed years for consistency.\n",
    "df = unfiltered_df[\n",
    "    (unfiltered_df[\"year\"] != 2025) & \n",
    "    (unfiltered_df[\"articleType\"].isin([\"article\", \"brief\"]))\n",
    "]\n",
    "\n",
    "\n",
    "# This has some errors/weirdness in its sections and title structure, filtering it out for now\n",
    "df = df[df[\"id\"] != \"10.1103/PhysRevSTPER.6.020117\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the XML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_sections(xml_content):\n",
    "    \"\"\"\n",
    "    Parse the XML content and extract section information.\n",
    "    \n",
    "    Args:\n",
    "        xml_content (str): The XML content as a string.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing section information.\n",
    "    \"\"\"\n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "    \n",
    "    sections = []\n",
    "    \n",
    "    # Find all sec elements in the body\n",
    "    for i,sec in enumerate(root.findall('.//body/sec')):\n",
    "        section_data = defaultdict(str)\n",
    "\n",
    "        section_data[\"relative_position\"] = i+1\n",
    "        \n",
    "        # Extract label\n",
    "        label = sec.find('label')\n",
    "        if label is not None:\n",
    "            section_data['label'] = label.text.strip() if label.text else ''\n",
    "        \n",
    "        # Extract title\n",
    "        title = sec.find('title')\n",
    "        if title is not None:\n",
    "            section_data['title'] = title.text.strip() if title.text else ''\n",
    "        \n",
    "        # Extract all text content from the section, runs recursively on the XML tree to extract all text content\n",
    "        def get_full_text(element):\n",
    "            \"\"\"Recursively get all text (including tails) from an element and its children.\"\"\"\n",
    "            texts = []\n",
    "            # If the element contains any text, collec\n",
    "            if element.text:\n",
    "                texts.append(element.text.strip())\n",
    "            for child in element:\n",
    "                # For xref, handle as reference\n",
    "                if child.tag == 'xref':\n",
    "                    ref = child.get('rid')\n",
    "                    ref_type = child.get('ref-type')\n",
    "                    if ref and ref_type:\n",
    "                        texts.append(f\"[{ref_type}:{ref}]\")\n",
    "                else:\n",
    "                    texts.append(get_full_text(child)) # If the element is not a reference, then process the element as its parent\n",
    "                if child.tail:\n",
    "                    texts.append(child.tail.strip())\n",
    "            return ' '.join([t for t in texts if t])\n",
    "\n",
    "        section_data['content'] = get_full_text(sec)\n",
    "\n",
    "        # Extract any other metadata like id\n",
    "        sec_id = sec.get('id')\n",
    "        if sec_id:\n",
    "            section_data['id'] = sec_id\n",
    "        \n",
    "        sections.append(dict(section_data))\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8586d88d700b4fe19bf20f04251c6313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"sections\"] = df[\"full_text_xml\"].progress_apply(parse_xml_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abstract', 'articleType', 'authors', 'affiliations', 'date', 'type',\n",
       "       'metadata_last_modified_at', 'last_modified_at', 'id', 'identifiers',\n",
       "       'issue', 'pageStart', 'hasArticleId', 'numPages', 'publisher', 'rights',\n",
       "       'journal', 'title', 'volume', 'notes', 'tocSection', 'fundings',\n",
       "       'classificationSchemes', 'doi', 'full_text_xml', 'full_text', 'year',\n",
       "       'sections'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode data for better access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the extracted data is saved in a dataframe with one row per article. Since we are interested in analysing the relation between different sections , it will be easier for us if our data was organized with one row per section. Therefore, we exlode the dataframe to get our desired format here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_sections(df, section_col=\"sections\"):\n",
    "    \"\"\"\n",
    "    Explodes a DataFrame on the given section_col (which should be a list of dicts per row),\n",
    "    and prefixes all original columns (except section_col) with 'article_'.\n",
    "    The resulting DataFrame will have one row per section, with section dict keys as columns,\n",
    "    and all original article-level columns prefixed.\n",
    "    \"\"\"\n",
    "    # Columns to keep and prefix (all except section_col)\n",
    "    article_cols = [col for col in df.columns if col != section_col]\n",
    "    # Prepare new column names\n",
    "    article_col_map = {col: f\"article_{col}\" for col in article_cols}\n",
    "    # Rename columns\n",
    "    df_prefixed = df.rename(columns=article_col_map)\n",
    "    # Explode the sections column\n",
    "    df_exploded = df_prefixed.explode(section_col, ignore_index=True)\n",
    "    # Expand the dict in the section_col into columns\n",
    "    section_df = pd.json_normalize(df_exploded[section_col])\n",
    "    section_df = section_df.rename(columns={col: f\"section_{col}\" for col in section_df.columns})\n",
    "    # Drop the old section_col and join the expanded section columns\n",
    "    df_exploded = df_exploded.drop(columns=[section_col])\n",
    "    result = pd.concat([df_exploded, section_df], axis=1)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "exploded_df = explode_sections(df, section_col=\"sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_abstract', 'article_articleType', 'article_authors',\n",
       "       'article_affiliations', 'article_date', 'article_type',\n",
       "       'article_metadata_last_modified_at', 'article_last_modified_at',\n",
       "       'article_id', 'article_identifiers', 'article_issue',\n",
       "       'article_pageStart', 'article_hasArticleId', 'article_numPages',\n",
       "       'article_publisher', 'article_rights', 'article_journal',\n",
       "       'article_title', 'article_volume', 'article_notes',\n",
       "       'article_tocSection', 'article_fundings',\n",
       "       'article_classificationSchemes', 'article_doi', 'article_full_text_xml',\n",
       "       'article_full_text', 'article_year', 'section_relative_position',\n",
       "       'section_label', 'section_title', 'section_content', 'section_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that all the text has been extracted properly, we run some basic validation comparing the text length of the full text and the extracted sections, as well as some checks on missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Section Data Analysis Report\n",
      "============================================================\n",
      "Total documents analyzed: 1222\n",
      "\n",
      "Documents without any sections:\n",
      "  Count: 0\n",
      "  Percentage: 0.00%\n",
      "  Document indices: []\n",
      "\n",
      "Total sections across all documents: 7313\n",
      "\n",
      "Sections without titles:\n",
      "  Count: 0\n",
      "  Percentage: 0.00%\n",
      "\n",
      "Sections without labels:\n",
      "  Count: 5\n",
      "  Percentage: 0.07%\n",
      "  Example sections without labels:\n",
      "    Document 126, Section 4: V. DISCUSSION AND CONCLUSION\n",
      "    Document 586, Section 0: Introduction.—\n",
      "    Document 586, Section 1: Methods.—\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Comparing text lengths\n",
      "--------------------------------------------------\n",
      "Total length of section_title: 146,307\n",
      "Total length of section_content: 72,932,071\n",
      "Sum of section_content and section_title: 73,078,378\n",
      "Original full text length: 72,167,341\n",
      "Difference (original - combined): -911,037\n",
      "\n",
      "\n",
      "Deviation between original full text and sum of parsed section contents:\n",
      "  Mean deviation:   -625.80 characters\n",
      "  Median deviation: -576.50 characters\n",
      "  Min deviation:    58.00 characters\n",
      "  Max deviation:    4762.00 characters\n",
      "  Number of articles analyzed: 1222\n"
     ]
    }
   ],
   "source": [
    "results = analyze_section_data_with_details(df[\"sections\"])\n",
    "print_analysis_report(results)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"-\"*50)\n",
    "print(\"Comparing text lengths\")\n",
    "print(\"-\"*50)\n",
    "compare_text_lengths(exploded_df, df)\n",
    "print(\"\\n\")\n",
    "res = analyze_section_content_vs_fulltext_deviation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sucsesfully extracted the sections, let us generate embeddings that we can use for further analysis. We have chosen to generate to forms of embeddings. First we embed the whole section as well as the titles. Second, we chunk the section text body into sentence chunks of up to 300 characters. This second embedding will be useful to explore the movement of articles/sections through the embedding space.\n",
    "\n",
    "For the embedding we use the `voyage-3-large` model provided through the VoyageAI API, it is currently the state-of-art embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding whole sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 rows in chunks of 50 for a total of 2 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding process for 50 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 50, estimated tokens: 266\n",
      "✓ Using maximum chunk size: 50\n",
      "Processing chunk 1: texts 0 to 49\n",
      "Completed: 1 chunks processed, 50 embeddings generated\n",
      "Starting embedding process for 50 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 50, estimated tokens: 88534\n",
      "✓ Using maximum chunk size: 50\n",
      "Processing chunk 1: texts 0 to 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 1 chunks processed, 50 embeddings generated\n",
      "Starting embedding process for 50 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 50, estimated tokens: 259\n",
      "✓ Using maximum chunk size: 50\n",
      "Processing chunk 1: texts 0 to 49\n",
      "Completed: 1 chunks processed, 50 embeddings generated\n",
      "Starting embedding process for 50 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 50, estimated tokens: 93903\n",
      "✓ Using maximum chunk size: 50\n",
      "Processing chunk 1: texts 0 to 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 2/2 [00:17<00:00,  8.58s/chunk, ETA: 0.0s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 1 chunks processed, 50 embeddings generated\n",
      "Completed processing in 17.2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used: 0\n"
     ]
    }
   ],
   "source": [
    "def embed_sections(rows):\n",
    "    results_df = rows.copy()\n",
    "\n",
    "    # Extract title and content from the expanded sections\n",
    "    titles = results_df['section_title'].tolist()\n",
    "    contents = results_df['section_content'].tolist()\n",
    "\n",
    "    # Process titles and contents using the helper function\n",
    "    # The embed_w_token_manager function reduces the number of \n",
    "    # requests to the API by finding the max amount of data that can be passed per request.\n",
    "    title_embeddings = embed_w_token_manager(titles, vo)\n",
    "    content_embeddings = embed_w_token_manager(contents, vo)\n",
    "\n",
    "    results_df[\"title_embedding\"] = title_embeddings\n",
    "    results_df[\"content_embedding\"] = content_embeddings\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "apply_in_chunks(exploded_df, embed_sections, chunk_size=100,\n",
    "                output_dir=\"chunks_whole_sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_embeddings = recombine_chunks(\"chunks_whole_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['section_label', 'article_title', 'section_title_raw', 'article_id',\n",
       "       'year', 'content_text', 'relative_position', 'title_embedding',\n",
       "       'content_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_abstract', 'article_articleType', 'article_authors',\n",
       "       'article_affiliations', 'article_date', 'article_type',\n",
       "       'article_metadata_last_modified_at', 'article_last_modified_at',\n",
       "       'article_id', 'article_identifiers', 'article_issue',\n",
       "       'article_pageStart', 'article_hasArticleId', 'article_numPages',\n",
       "       'article_publisher', 'article_rights', 'article_journal',\n",
       "       'article_title', 'article_volume', 'article_notes',\n",
       "       'article_tocSection', 'article_fundings',\n",
       "       'article_classificationSchemes', 'article_doi', 'article_full_text_xml',\n",
       "       'article_full_text', 'article_year', 'section_relative_position',\n",
       "       'section_label', 'section_title', 'section_content', 'section_id',\n",
       "       'section_title_embedding', 'section_content_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the relevant embedding columns from df_with_embeddings\n",
    "embeddings_to_merge = df_with_embeddings[[\"article_id\", \"section_title_raw\", \"title_embedding\", \"content_embedding\"]].copy()\n",
    "embeddings_to_merge = embeddings_to_merge.rename(\n",
    "    columns={\n",
    "        \"title_embedding\": \"section_title_embedding\",\n",
    "        \"content_embedding\": \"section_content_embedding\",\n",
    "        \"section_title_raw\": \"section_title\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Merge into exploded_df, keeping all columns from exploded_df and only the two embedding columns from embeddings_to_merge\n",
    "df_with_embeddings = exploded_df.merge(\n",
    "    embeddings_to_merge,\n",
    "    on=[\"article_id\", \"section_title\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "df_with_embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the embeddings were processed correctly, we run a semantic similarity test on a random sample of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_semantic_consistency(text, embedding, vo, similarity_threshold=0.7, print_similarity=False):\n",
    "    \"\"\"\n",
    "    Verify that the embedding semantically represents the original text\n",
    "    by comparing it with a fresh embedding of the same text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a fresh embedding for comparison\n",
    "    verification_embedding = vo.embed([text], model=\"voyage-3-large\").embeddings[0]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = 1 - cosine(embedding, verification_embedding)\n",
    "    \n",
    "    # Print similarity if requested\n",
    "    if print_similarity:\n",
    "        print(f\"Similarity: {similarity:.3f}\")\n",
    "    \n",
    "    if similarity < similarity_threshold:\n",
    "        print(f\"❌ Low semantic similarity ({similarity:.3f}) for chunk: {text[:100]}...\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying semantic consistency for 10 random samples...\n",
      "============================================================\n",
      "\n",
      "Sample 6380:\n",
      "Text: II. THEORETICAL FRAMEWORK A. Relationships and relationship building As individuals engage with each...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 5966:\n",
      "Text: III. RESEARCH QUESTION Our study wanted to understand instructors’ views around cultural relevance, ...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 858:\n",
      "Text: III. RESEARCH A. Aims and research questions This study is based on quantitative and qualitative res...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 4585:\n",
      "Text: II. DESIGN-BASED RESEARCH: INTEGRATING EVIDENCE-BASED, THEORY-DRIVEN, AND PRACTICE-BASED ASPECTS INT...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 2086:\n",
      "Text: IV. RESULTS Our first research question asked how students characterize professionals in their chose...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 5155:\n",
      "Text: I. INTRODUCTION A central goal of most physics education research (PER) is to find ways to teach bet...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 4483:\n",
      "Text: I. INTRODUCTION It is readily noticed and well documented that women, black or African Americans, Hi...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 3507:\n",
      "Text: V. RESULTS AND DISCUSSION No group was able to construct a correct differential length element on th...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 1416:\n",
      "Text: III. SETTING FOR THE CASE STUDY A. Energy is an ideal context for studying interdisciplinary reconci...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Sample 1426:\n",
      "Text: VI. CONLCUSION Sociotechnological classrooms are designed to promote student-centered learning. Soci...\n",
      "Title embedding consistent: ✅\n",
      "Content embedding consistent: ✅\n",
      "----------------------------------------\n",
      "\n",
      "Verification complete!\n"
     ]
    }
   ],
   "source": [
    "# Sample a few rows for verification\n",
    "sample_size = 10\n",
    "sample_df = df_with_embeddings.sample(n=sample_size)\n",
    "\n",
    "print(f\"Verifying semantic consistency for {sample_size} random samples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"Text: {row['section_content'][:100]}...\")\n",
    "    \n",
    "    # Verify title embedding\n",
    "    title_consistent = verify_semantic_consistency(\n",
    "        row['section_title'], \n",
    "        row['section_title_embedding'], \n",
    "        vo\n",
    "    )\n",
    "    print(f\"Title embedding consistent: {'✅' if title_consistent else '❌'}\")\n",
    "    \n",
    "    # Verify content embedding\n",
    "    content_consistent = verify_semantic_consistency(\n",
    "        row['section_content'], \n",
    "        row['section_content_embedding'], \n",
    "        vo\n",
    "    )\n",
    "    print(f\"Content embedding consistent: {'✅' if content_consistent else '❌'}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nVerification complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the fruits of our hard labour. We save this first result containing only the embeddigns for the whole sections as a \"light\" version, this will be useful later on when running analysis only on the whole sections. Once we add the chunked sections and its embeddings, the saved `.pkl` file will baloon to approx. 3GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_code.helpers import save_processed_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed_embeddings to ./processed_embeddings_light_20250715_132106.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./processed_embeddings_light_20250715_132106.pkl'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the embeddings\n",
    "save_processed_embeddings(df_with_embeddings, \"processed_embeddings_light\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding chuncked sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_abstract', 'article_articleType', 'article_authors',\n",
       "       'article_affiliations', 'article_date', 'article_type',\n",
       "       'article_metadata_last_modified_at', 'article_last_modified_at',\n",
       "       'article_id', 'article_identifiers', 'article_issue',\n",
       "       'article_pageStart', 'article_hasArticleId', 'article_numPages',\n",
       "       'article_publisher', 'article_rights', 'article_journal',\n",
       "       'article_title', 'article_volume', 'article_notes',\n",
       "       'article_tocSection', 'article_fundings',\n",
       "       'article_classificationSchemes', 'article_doi', 'article_full_text_xml',\n",
       "       'article_full_text', 'article_year', 'section_relative_position',\n",
       "       'section_label', 'section_title', 'section_content', 'section_id',\n",
       "       'section_title_embedding', 'section_content_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally load the df generated above to run this section of the notebook in isolation\n",
    "\n",
    "# df_with_embeddings = pd.read_pickle(\"processed_embeddings_light_20250715_132106.pkl\")\n",
    "# df_with_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sfgar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download Punkt tokenizer (first time only)\n",
    "download('punkt')\n",
    "\n",
    "def nltk_sentence_splitter(text, min_chunk=50, max_chunk=300):\n",
    "    \"\"\"NLTK implementation with chunk size control\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_len = len(sent)\n",
    "        # Start new chunk if adding this sentence would exceed max size\n",
    "        if current_length + sent_len > max_chunk and current_length >= min_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sent]\n",
    "            current_length = sent_len\n",
    "        else:\n",
    "            current_chunk.append(sent)\n",
    "            current_length += sent_len\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_section(section_text):\n",
    "    return nltk_sentence_splitter(section_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7313 rows in chunks of 200 for a total of 37 chunks...\n",
      "Skipping chunk 0-199, already exists.\n",
      "Skipping chunk 200-399, already exists.\n",
      "Processing chunk 400-599...\n",
      "505909\n",
      "Starting embedding process for 8562 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 47924\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 47010\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48650\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 51845\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 46354\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 51894\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 52329\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 45440\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 562, estimated tokens: 26507\n",
      "✓ Using maximum chunk size: 562\n",
      "Processing chunk 9: texts 8000 to 8561\n",
      "Completed: 9 chunks processed, 8562 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_400_599.pkl\n",
      "Time estimate: 843.9 seconds remaining\n",
      "Processing chunk 600-799...\n",
      "490333\n",
      "Starting embedding process for 8253 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 48600\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 49411\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 53128\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49068\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 48966\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 55122\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 52990\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 46835\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 253, estimated tokens: 12413\n",
      "✓ Using maximum chunk size: 253\n",
      "Processing chunk 9: texts 8000 to 8252\n",
      "Completed: 9 chunks processed, 8253 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_600_799.pkl\n",
      "Time estimate: 1288.7 seconds remaining\n",
      "Processing chunk 800-999...\n",
      "480693\n",
      "Starting embedding process for 8084 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 54056\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 53623\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 49457\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 47621\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50029\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50794\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 51250\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 44247\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 84, estimated tokens: 3580\n",
      "✓ Using maximum chunk size: 84\n",
      "Processing chunk 9: texts 8000 to 8083\n",
      "Completed: 9 chunks processed, 8084 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_800_999.pkl\n",
      "Time estimate: 1503.2 seconds remaining\n",
      "Processing chunk 1000-1199...\n",
      "532295\n",
      "Starting embedding process for 9067 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49374\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 46607\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48295\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50889\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 49428\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 47846\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 52819\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 48934\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 1000, estimated tokens: 50058\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 9: texts 8000 to 8999\n",
      "Position 9000: Trying chunk size 67, estimated tokens: 2519\n",
      "✓ Using maximum chunk size: 67\n",
      "Processing chunk 10: texts 9000 to 9066\n",
      "Completed: 10 chunks processed, 9067 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_1000_1199.pkl\n",
      "Time estimate: 1546.9 seconds remaining\n",
      "Processing chunk 1200-1399...\n",
      "447772\n",
      "Starting embedding process for 7537 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 51428\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50241\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 44906\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50064\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 48718\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 48463\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 49096\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 537, estimated tokens: 26891\n",
      "✓ Using maximum chunk size: 537\n",
      "Processing chunk 8: texts 7000 to 7536\n",
      "Completed: 8 chunks processed, 7537 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_1200_1399.pkl\n",
      "Time estimate: 1513.2 seconds remaining\n",
      "Processing chunk 1400-1599...\n",
      "474553\n",
      "Starting embedding process for 8024 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 51586\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50029\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 54248\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50116\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 48722\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 46343\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 52386\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 52067\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 24, estimated tokens: 1020\n",
      "✓ Using maximum chunk size: 24\n",
      "Processing chunk 9: texts 8000 to 8023\n",
      "Completed: 9 chunks processed, 8024 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_1400_1599.pkl\n",
      "Time estimate: 1598.6 seconds remaining\n",
      "Processing chunk 1600-1799...\n",
      "471268\n",
      "Starting embedding process for 8000 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 46521\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 46981\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 50351\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 51800\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50670\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49507\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50553\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 50193\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Completed: 8 chunks processed, 8000 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_1600_1799.pkl\n",
      "Time estimate: 1544.4 seconds remaining\n",
      "Processing chunk 1800-1999...\n",
      "507853\n",
      "Starting embedding process for 8359 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 55231\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52908\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48646\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 51792\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 52774\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 54096\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 49819\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 51575\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 359, estimated tokens: 16229\n",
      "✓ Using maximum chunk size: 359\n",
      "Processing chunk 9: texts 8000 to 8358\n",
      "Completed: 9 chunks processed, 8359 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_1800_1999.pkl\n",
      "Time estimate: 1468.6 seconds remaining\n",
      "Processing chunk 2000-2199...\n",
      "429297\n",
      "Starting embedding process for 7228 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49320\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50241\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 53669\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50414\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 52060\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 48795\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50424\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 228, estimated tokens: 11182\n",
      "✓ Using maximum chunk size: 228\n",
      "Processing chunk 8: texts 7000 to 7227\n",
      "Completed: 8 chunks processed, 7228 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_2000_2199.pkl\n",
      "Time estimate: 1448.6 seconds remaining\n",
      "Processing chunk 2200-2399...\n",
      "500408\n",
      "Starting embedding process for 8296 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 50229\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 51219\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 53584\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 61193\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 57170\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50318\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 51227\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 47311\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 296, estimated tokens: 18050\n",
      "✓ Using maximum chunk size: 296\n",
      "Processing chunk 9: texts 8000 to 8295\n",
      "Completed: 9 chunks processed, 8296 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_2200_2399.pkl\n",
      "Time estimate: 1445.8 seconds remaining\n",
      "Processing chunk 2400-2599...\n",
      "497015\n",
      "Starting embedding process for 8395 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 51571\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 51178\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 49667\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 48640\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 47932\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49701\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50557\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 49084\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 395, estimated tokens: 18286\n",
      "✓ Using maximum chunk size: 395\n",
      "Processing chunk 9: texts 8000 to 8394\n",
      "Completed: 9 chunks processed, 8395 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_2400_2599.pkl\n",
      "Time estimate: 1437.9 seconds remaining\n",
      "Processing chunk 2600-2799...\n",
      "541890\n",
      "Starting embedding process for 9097 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 56223\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 55272\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 51859\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 59162\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 53082\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 47824\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 51281\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 48871\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 1000, estimated tokens: 50389\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 9: texts 8000 to 8999\n",
      "Position 9000: Trying chunk size 97, estimated tokens: 4479\n",
      "✓ Using maximum chunk size: 97\n",
      "Processing chunk 10: texts 9000 to 9096\n",
      "Completed: 10 chunks processed, 9097 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_2600_2799.pkl\n",
      "Time estimate: 1409.9 seconds remaining\n",
      "Processing chunk 2800-2999...\n",
      "446942\n",
      "Starting embedding process for 7575 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 52325\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50631\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48289\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50093\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 52777\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50667\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 55535\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 575, estimated tokens: 28017\n",
      "✓ Using maximum chunk size: 575\n",
      "Processing chunk 8: texts 7000 to 7574\n",
      "Completed: 8 chunks processed, 7575 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_2800_2999.pkl\n",
      "Time estimate: 1329.5 seconds remaining\n",
      "Processing chunk 3000-3199...\n",
      "518575\n",
      "Starting embedding process for 8836 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 48674\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52167\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 50361\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 53318\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 49994\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 55636\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 49142\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 50696\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 836, estimated tokens: 39861\n",
      "✓ Using maximum chunk size: 836\n",
      "Processing chunk 9: texts 8000 to 8835\n",
      "Completed: 9 chunks processed, 8836 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_3000_3199.pkl\n",
      "Time estimate: 1258.0 seconds remaining\n",
      "Processing chunk 3200-3399...\n",
      "490718\n",
      "Starting embedding process for 8280 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49057\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 51232\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 49746\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 55557\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50801\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49465\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 55682\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 46076\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 280, estimated tokens: 12846\n",
      "✓ Using maximum chunk size: 280\n",
      "Processing chunk 9: texts 8000 to 8279\n",
      "Completed: 9 chunks processed, 8280 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_3200_3399.pkl\n",
      "Time estimate: 1195.7 seconds remaining\n",
      "Processing chunk 3400-3599...\n",
      "457155\n",
      "Starting embedding process for 7700 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49230\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50366\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 52189\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 57287\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 51943\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50805\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 51832\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 700, estimated tokens: 34228\n",
      "✓ Using maximum chunk size: 700\n",
      "Processing chunk 8: texts 7000 to 7699\n",
      "Completed: 8 chunks processed, 7700 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_3400_3599.pkl\n",
      "Time estimate: 1127.5 seconds remaining\n",
      "Processing chunk 3600-3799...\n",
      "463452\n",
      "Starting embedding process for 7800 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 50584\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50045\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 50356\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49611\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 46949\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 48874\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 54416\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 800, estimated tokens: 37395\n",
      "✓ Using maximum chunk size: 800\n",
      "Processing chunk 8: texts 7000 to 7799\n",
      "Completed: 8 chunks processed, 7800 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_3600_3799.pkl\n",
      "Time estimate: 1052.1 seconds remaining\n",
      "Processing chunk 3800-3999...\n",
      "573574\n",
      "Starting embedding process for 9630 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 56592\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 53649\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48544\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49528\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 48583\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49953\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 46374\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 51077\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 1000, estimated tokens: 58857\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 9: texts 8000 to 8999\n",
      "Position 9000: Trying chunk size 630, estimated tokens: 31844\n",
      "✓ Using maximum chunk size: 630\n",
      "Processing chunk 10: texts 9000 to 9629\n",
      "Completed: 10 chunks processed, 9630 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_3800_3999.pkl\n",
      "Time estimate: 999.1 seconds remaining\n",
      "Processing chunk 4000-4199...\n",
      "600392\n",
      "Starting embedding process for 10173 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 50027\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52074\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 46311\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 45583\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 45987\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 46412\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 46547\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 47241\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 1000, estimated tokens: 46072\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 9: texts 8000 to 8999\n",
      "Position 9000: Trying chunk size 1000, estimated tokens: 47714\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 10: texts 9000 to 9999\n",
      "Position 10000: Trying chunk size 173, estimated tokens: 8108\n",
      "✓ Using maximum chunk size: 173\n",
      "Processing chunk 11: texts 10000 to 10172\n",
      "Completed: 11 chunks processed, 10173 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_4000_4199.pkl\n",
      "Time estimate: 945.2 seconds remaining\n",
      "Processing chunk 4200-4399...\n",
      "436678\n",
      "Starting embedding process for 7405 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 54905\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 57471\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 52931\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50811\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 52771\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 51159\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 48648\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 405, estimated tokens: 22461\n",
      "✓ Using maximum chunk size: 405\n",
      "Processing chunk 8: texts 7000 to 7404\n",
      "Completed: 8 chunks processed, 7405 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_4200_4399.pkl\n",
      "Time estimate: 876.1 seconds remaining\n",
      "Processing chunk 4400-4599...\n",
      "522597\n",
      "Starting embedding process for 8808 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49742\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52319\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 50032\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50826\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 56321\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 52581\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 52112\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 52881\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 808, estimated tokens: 36504\n",
      "✓ Using maximum chunk size: 808\n",
      "Processing chunk 9: texts 8000 to 8807\n",
      "Completed: 9 chunks processed, 8808 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_4400_4599.pkl\n",
      "Time estimate: 817.8 seconds remaining\n",
      "Processing chunk 4600-4799...\n",
      "482933\n",
      "Starting embedding process for 8113 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 52156\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 51356\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 53554\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 48801\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 46708\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49469\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 49619\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 51231\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 113, estimated tokens: 6866\n",
      "✓ Using maximum chunk size: 113\n",
      "Processing chunk 9: texts 8000 to 8112\n",
      "Completed: 9 chunks processed, 8113 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_4600_4799.pkl\n",
      "Time estimate: 756.3 seconds remaining\n",
      "Processing chunk 4800-4999...\n",
      "517332\n",
      "Starting embedding process for 8823 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49347\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 47034\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 49586\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 53387\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50613\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 51120\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50574\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 50086\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 823, estimated tokens: 37741\n",
      "✓ Using maximum chunk size: 823\n",
      "Processing chunk 9: texts 8000 to 8822\n",
      "Completed: 9 chunks processed, 8823 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_4800_4999.pkl\n",
      "Time estimate: 701.8 seconds remaining\n",
      "Processing chunk 5000-5199...\n",
      "474866\n",
      "Starting embedding process for 7962 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 54816\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50069\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48418\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 54202\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 53311\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 48731\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 53470\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 962, estimated tokens: 53797\n",
      "✓ Using maximum chunk size: 962\n",
      "Processing chunk 8: texts 7000 to 7961\n",
      "Completed: 8 chunks processed, 7962 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_5000_5199.pkl\n",
      "Time estimate: 641.6 seconds remaining\n",
      "Processing chunk 5200-5399...\n",
      "517197\n",
      "Starting embedding process for 8767 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 47334\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50172\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 46524\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49128\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 51524\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50933\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50875\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 49940\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 767, estimated tokens: 42772\n",
      "✓ Using maximum chunk size: 767\n",
      "Processing chunk 9: texts 8000 to 8766\n",
      "Completed: 9 chunks processed, 8767 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_5200_5399.pkl\n",
      "Time estimate: 585.0 seconds remaining\n",
      "Processing chunk 5400-5599...\n",
      "519488\n",
      "Starting embedding process for 8715 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 54266\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 45531\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 47012\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 48674\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50091\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 58268\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 46057\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 47095\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 715, estimated tokens: 34566\n",
      "✓ Using maximum chunk size: 715\n",
      "Processing chunk 9: texts 8000 to 8714\n",
      "Completed: 9 chunks processed, 8715 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_5400_5599.pkl\n",
      "Time estimate: 525.7 seconds remaining\n",
      "Processing chunk 5600-5799...\n",
      "462624\n",
      "Starting embedding process for 7862 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 55380\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50703\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 48334\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 48905\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 49750\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 46806\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 47992\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 862, estimated tokens: 42642\n",
      "✓ Using maximum chunk size: 862\n",
      "Processing chunk 8: texts 7000 to 7861\n",
      "Completed: 8 chunks processed, 7862 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_5600_5799.pkl\n",
      "Time estimate: 459.7 seconds remaining\n",
      "Processing chunk 5800-5999...\n",
      "493662\n",
      "Starting embedding process for 8270 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49422\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 51635\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 51756\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 51973\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50215\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49010\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 45784\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 52864\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 270, estimated tokens: 12420\n",
      "✓ Using maximum chunk size: 270\n",
      "Processing chunk 9: texts 8000 to 8269\n",
      "Completed: 9 chunks processed, 8270 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_5800_5999.pkl\n",
      "Time estimate: 396.6 seconds remaining\n",
      "Processing chunk 6000-6199...\n",
      "525251\n",
      "Starting embedding process for 8781 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49618\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52604\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 50127\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 47587\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 55810\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50593\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 51516\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 46854\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 781, estimated tokens: 40694\n",
      "✓ Using maximum chunk size: 781\n",
      "Processing chunk 9: texts 8000 to 8780\n",
      "Completed: 9 chunks processed, 8781 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_6000_6199.pkl\n",
      "Time estimate: 335.6 seconds remaining\n",
      "Processing chunk 6200-6399...\n",
      "523622\n",
      "Starting embedding process for 8923 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 48216\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 48968\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 49703\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49307\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 54876\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 45956\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 48208\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 48829\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 923, estimated tokens: 44804\n",
      "✓ Using maximum chunk size: 923\n",
      "Processing chunk 9: texts 8000 to 8922\n",
      "Completed: 9 chunks processed, 8923 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_6200_6399.pkl\n",
      "Time estimate: 275.7 seconds remaining\n",
      "Processing chunk 6400-6599...\n",
      "497905\n",
      "Starting embedding process for 8357 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 50746\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 50429\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 47797\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50037\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 52055\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 50999\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 46343\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 54147\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 357, estimated tokens: 17632\n",
      "✓ Using maximum chunk size: 357\n",
      "Processing chunk 9: texts 8000 to 8356\n",
      "Completed: 9 chunks processed, 8357 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_6400_6599.pkl\n",
      "Time estimate: 217.9 seconds remaining\n",
      "Processing chunk 6600-6799...\n",
      "577234\n",
      "Starting embedding process for 9667 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49355\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 48502\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 51384\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 46776\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 47790\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49363\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 53504\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 52560\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 1000, estimated tokens: 51108\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 9: texts 8000 to 8999\n",
      "Position 9000: Trying chunk size 667, estimated tokens: 32010\n",
      "✓ Using maximum chunk size: 667\n",
      "Processing chunk 10: texts 9000 to 9666\n",
      "Completed: 10 chunks processed, 9667 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_6600_6799.pkl\n",
      "Time estimate: 161.9 seconds remaining\n",
      "Processing chunk 6800-6999...\n",
      "516333\n",
      "Starting embedding process for 8695 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 51193\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 49020\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 52783\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 47591\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 50160\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 52490\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 48914\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 48090\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 695, estimated tokens: 34905\n",
      "✓ Using maximum chunk size: 695\n",
      "Processing chunk 9: texts 8000 to 8694\n",
      "Completed: 9 chunks processed, 8695 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_6800_6999.pkl\n",
      "Time estimate: 106.7 seconds remaining\n",
      "Processing chunk 7000-7199...\n",
      "501137\n",
      "Starting embedding process for 8294 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 54116\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 49476\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 54674\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 50104\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 1000, estimated tokens: 54416\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 5: texts 4000 to 4999\n",
      "Position 5000: Trying chunk size 1000, estimated tokens: 49898\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 6: texts 5000 to 5999\n",
      "Position 6000: Trying chunk size 1000, estimated tokens: 50674\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 7: texts 6000 to 6999\n",
      "Position 7000: Trying chunk size 1000, estimated tokens: 50879\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 8: texts 7000 to 7999\n",
      "Position 8000: Trying chunk size 294, estimated tokens: 12888\n",
      "✓ Using maximum chunk size: 294\n",
      "Processing chunk 9: texts 8000 to 8293\n",
      "Completed: 9 chunks processed, 8294 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_7000_7199.pkl\n",
      "Time estimate: 52.8 seconds remaining\n",
      "Processing chunk 7200-7312...\n",
      "244424\n",
      "Starting embedding process for 4201 texts with max_tokens=120000\n",
      "Position 0: Trying chunk size 1000, estimated tokens: 49636\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 1: texts 0 to 999\n",
      "Position 1000: Trying chunk size 1000, estimated tokens: 52421\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 2: texts 1000 to 1999\n",
      "Position 2000: Trying chunk size 1000, estimated tokens: 47387\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 3: texts 2000 to 2999\n",
      "Position 3000: Trying chunk size 1000, estimated tokens: 49764\n",
      "✓ Using maximum chunk size: 1000\n",
      "Processing chunk 4: texts 3000 to 3999\n",
      "Position 4000: Trying chunk size 201, estimated tokens: 9495\n",
      "✓ Using maximum chunk size: 201\n",
      "Processing chunk 5: texts 4000 to 4200\n",
      "Completed: 5 chunks processed, 4201 embeddings generated\n",
      "Saved chunk to chunks_chunked_sections\\chunk_7200_7312.pkl\n",
      "Time estimate: 0.0 seconds remaining\n",
      "Total tokens used: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def embed_section(df_chunk):\n",
    "    # Process all sections in the dataframe chunk\n",
    "    all_chunks = []\n",
    "    chunk_mapping = []  # Track which chunk belongs to which section\n",
    "    \n",
    "    for idx, section in df_chunk.iterrows():\n",
    "        section_text = section[\"section_content\"]\n",
    "        assert len(section_text) > 20, \"Section text is too short\"\n",
    "\n",
    "        # Chunk the section\n",
    "        chunked_contents = chunk_section(section_text)\n",
    "        \n",
    "        # Add chunks to the aggregated list\n",
    "        for chunk in chunked_contents:\n",
    "            all_chunks.append(chunk)\n",
    "            chunk_mapping.append(idx)  # Track which section this chunk belongs to\n",
    "    print(sum(len(chunk) for chunk in all_chunks)//4)\n",
    "    # Embed all chunks using the custom token manager function\n",
    "    embedded_chunks = embed_w_token_manager(all_chunks, vo, max_tokens=120000, model=\"voyage-3-large\")\n",
    "    \n",
    "    # Resplit the aggregated embeddings back into sections\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    result_df = df_chunk.copy()\n",
    "    \n",
    "    # Create dictionaries to store the embeddings and text chunks for each section\n",
    "    section_embeddings_dict = {}\n",
    "    section_text_chunks_dict = {}\n",
    "    current_idx = 0\n",
    "    for section_idx in df_chunk.index:\n",
    "        section_chunks = []\n",
    "        section_text_chunks = []\n",
    "        # Find all chunks that belong to this section\n",
    "        while current_idx < len(chunk_mapping) and chunk_mapping[current_idx] == section_idx:\n",
    "            section_chunks.append(embedded_chunks[current_idx])\n",
    "            section_text_chunks.append(all_chunks[current_idx])\n",
    "            current_idx += 1\n",
    "        section_embeddings_dict[section_idx] = section_chunks\n",
    "        section_text_chunks_dict[section_idx] = section_text_chunks\n",
    "    \n",
    "    # Add the new columns all at once using the dictionaries\n",
    "    result_df['section_chunked_embeddings'] = result_df.index.map(section_embeddings_dict)\n",
    "    result_df['section_chunked_texts'] = result_df.index.map(section_text_chunks_dict)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "apply_in_chunks(df_with_embeddings, embed_section, chunk_size=200, output_dir=\"chunks_chunked_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/2 [18:25<?, ?chunk/s, Processing chunk 0-49]\n"
     ]
    }
   ],
   "source": [
    "df_all_embeddings = recombine_chunks(\"chunks_chunked_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['section_label', 'article_title', 'section_title_raw', 'article_id',\n",
       "       'year', 'content_text', 'relative_position', 'title_embedding',\n",
       "       'content_embedding', 'chunked_section_embeddings',\n",
       "       'chunked_section_texts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the chuncks by similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the similarity of a sample of sections chunked embeddings with a similar logic to above. This makes sure that the embedding was carried out correctly, and that there was no mismatch between text and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 3 sections for semantic consistency verification...\n",
      "\n",
      "Verifying section 2584 with 7 chunks...\n",
      "  ✓ Chunk 0 verified\n",
      "  ✓ Chunk 1 verified\n",
      "  ✓ Chunk 2 verified\n",
      "  ✓ Chunk 3 verified\n",
      "  ✓ Chunk 4 verified\n",
      "  ✓ Chunk 5 verified\n",
      "  ✓ Chunk 6 verified\n",
      "\n",
      "Verifying section 3454 with 36 chunks...\n",
      "  ✓ Chunk 0 verified\n",
      "  ✓ Chunk 1 verified\n",
      "  ✓ Chunk 2 verified\n",
      "  ✓ Chunk 3 verified\n",
      "  ✓ Chunk 4 verified\n",
      "  ✓ Chunk 5 verified\n",
      "  ✓ Chunk 6 verified\n",
      "  ✓ Chunk 7 verified\n",
      "  ✓ Chunk 8 verified\n",
      "  ✓ Chunk 9 verified\n",
      "  ✓ Chunk 10 verified\n",
      "  ✓ Chunk 11 verified\n",
      "  ✓ Chunk 12 verified\n",
      "  ✓ Chunk 13 verified\n",
      "  ✓ Chunk 14 verified\n",
      "  ✓ Chunk 15 verified\n",
      "  ✓ Chunk 16 verified\n",
      "  ✓ Chunk 17 verified\n",
      "  ✓ Chunk 18 verified\n",
      "  ✓ Chunk 19 verified\n",
      "  ✓ Chunk 20 verified\n",
      "  ✓ Chunk 21 verified\n",
      "  ✓ Chunk 22 verified\n",
      "  ✓ Chunk 23 verified\n",
      "  ✓ Chunk 24 verified\n",
      "  ✓ Chunk 25 verified\n",
      "  ✓ Chunk 26 verified\n",
      "  ✓ Chunk 27 verified\n",
      "  ✓ Chunk 28 verified\n",
      "  ✓ Chunk 29 verified\n",
      "  ✓ Chunk 30 verified\n",
      "  ✓ Chunk 31 verified\n",
      "  ✓ Chunk 32 verified\n",
      "  ✓ Chunk 33 verified\n",
      "  ✓ Chunk 34 verified\n",
      "  ✓ Chunk 35 verified\n",
      "\n",
      "Verifying section 3437 with 49 chunks...\n",
      "  ✓ Chunk 0 verified\n",
      "  ✓ Chunk 1 verified\n",
      "  ✓ Chunk 2 verified\n",
      "  ✓ Chunk 3 verified\n",
      "  ✓ Chunk 4 verified\n",
      "  ✓ Chunk 5 verified\n",
      "  ✓ Chunk 6 verified\n",
      "  ✓ Chunk 7 verified\n",
      "  ✓ Chunk 8 verified\n",
      "  ✓ Chunk 9 verified\n",
      "  ✓ Chunk 10 verified\n",
      "  ✓ Chunk 11 verified\n",
      "  ✓ Chunk 12 verified\n",
      "  ✓ Chunk 13 verified\n",
      "  ✓ Chunk 14 verified\n",
      "  ✓ Chunk 15 verified\n",
      "  ✓ Chunk 16 verified\n",
      "  ✓ Chunk 17 verified\n",
      "  ✓ Chunk 18 verified\n",
      "  ✓ Chunk 19 verified\n",
      "  ✓ Chunk 20 verified\n",
      "  ✓ Chunk 21 verified\n",
      "  ✓ Chunk 22 verified\n",
      "  ✓ Chunk 23 verified\n",
      "  ✓ Chunk 24 verified\n",
      "  ✓ Chunk 25 verified\n",
      "  ✓ Chunk 26 verified\n",
      "  ✓ Chunk 27 verified\n",
      "  ✓ Chunk 28 verified\n",
      "  ✓ Chunk 29 verified\n",
      "  ✓ Chunk 30 verified\n",
      "  ✓ Chunk 31 verified\n",
      "  ✓ Chunk 32 verified\n",
      "  ✓ Chunk 33 verified\n",
      "  ✓ Chunk 34 verified\n",
      "  ✓ Chunk 35 verified\n",
      "  ✓ Chunk 36 verified\n",
      "  ✓ Chunk 37 verified\n",
      "  ✓ Chunk 38 verified\n",
      "  ✓ Chunk 39 verified\n",
      "  ✓ Chunk 40 verified\n",
      "  ✓ Chunk 41 verified\n",
      "  ✓ Chunk 42 verified\n",
      "  ✓ Chunk 43 verified\n",
      "  ✓ Chunk 44 verified\n",
      "  ✓ Chunk 45 verified\n",
      "  ✓ Chunk 46 verified\n",
      "  ✓ Chunk 47 verified\n",
      "  ✓ Chunk 48 verified\n",
      "\n",
      "=== Verification Summary ===\n",
      "Total chunks verified: 92\n",
      "Failed chunks: 0\n",
      "Success rate: 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_some_chunks(df, vo, sample_size=10, random_state=None, print_similarity=False):\n",
    "    \"\"\"\n",
    "    Sample sections from dataframe and verify semantic consistency of their chunks.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with 'chunked_section_texts' and 'chunked_section_embeddings' columns\n",
    "        vo: VoyageAI client instance\n",
    "        sample_size: Number of sections to sample for verification\n",
    "        random_state: Random seed for reproducible sampling\n",
    "    \"\"\"\n",
    "    print(f\"Sampling {sample_size} sections for semantic consistency verification...\")\n",
    "    \n",
    "    # Sample sections\n",
    "    sampled_sections = df.sample(n=min(sample_size, len(df)), random_state=random_state)\n",
    "    \n",
    "    total_chunks_verified = 0\n",
    "    failed_chunks = 0\n",
    "    \n",
    "    for idx, row in sampled_sections.iterrows():\n",
    "        section_texts = row['section_chunked_texts']\n",
    "        section_embeddings = row['section_chunked_embeddings']\n",
    "        \n",
    "        print(f\"\\nVerifying section {idx} with {len(section_texts)} chunks...\")\n",
    "        \n",
    "        for i, (chunk_text, chunk_embedding) in enumerate(zip(section_texts, section_embeddings)):\n",
    "            total_chunks_verified += 1\n",
    "            if not verify_semantic_consistency(chunk_text, chunk_embedding, vo, print_similarity=print_similarity):\n",
    "                failed_chunks += 1\n",
    "                print(f\"  ❌ Chunk {i} failed verification\")\n",
    "            else:\n",
    "                print(f\"  ✓ Chunk {i} verified\")\n",
    "    \n",
    "    print(\"\\n=== Verification Summary ===\")\n",
    "    print(f\"Total chunks verified: {total_chunks_verified}\")\n",
    "    print(f\"Failed chunks: {failed_chunks}\")\n",
    "    print(f\"Success rate: {((total_chunks_verified - failed_chunks) / total_chunks_verified * 100):.1f}%\")\n",
    "    \n",
    "    return failed_chunks == 0\n",
    "\n",
    "res = verify_some_chunks(df_all_embeddings, vo, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_processed_embeddings(df_all_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inted_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
